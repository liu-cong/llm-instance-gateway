---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.16.1
  name: inferencemodels.inference.networking.x-k8s.io
spec:
  group: inference.networking.x-k8s.io
  names:
    kind: InferenceModel
    listKind: InferenceModelList
    plural: inferencemodels
    singular: inferencemodel
  scope: Namespaced
  versions:
  - name: v1alpha1
    schema:
      openAPIV3Schema:
        description: InferenceModel is the Schema for the inferencemodels API
        properties:
          apiVersion:
            description: |-
              APIVersion defines the versioned schema of this representation of an object.
              Servers should convert recognized schemas to the latest internal value, and
              may reject unrecognized values.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
            type: string
          kind:
            description: |-
              Kind is a string value representing the REST resource this object represents.
              Servers may infer this from the endpoint the client submits requests to.
              Cannot be updated.
              In CamelCase.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
            type: string
          metadata:
            type: object
          spec:
            description: |-
              InferenceModel represents a set of LLM services that are multiplexed onto one
              or more InferencePools. This resource is managed by the "LLM Service Owner"
              persona. The Service Owner persona is: a team that trains, verifies, and
              leverages a large language model from a model frontend, drives the lifecycle
              and rollout of new versions of those models, and defines the specific
              performance and latency goals for the model. These services are
              expected to operate within a InferencePool sharing compute capacity with other
              InferenceModels, defined by the Inference Platform Admin. We allow a user who
              has multiple InferenceModels across multiple pools (with the same config) to
              specify the configuration exactly once, and deploy to many pools
              simultaneously. Enabling a simpler config and single source of truth
              for a given user. InferenceModel names are unique for a given InferencePool,
              if the name is reused, an error will be  shown on the status of a
              InferenceModel that attempted to reuse. The oldest InferenceModel, based on
              creation timestamp, will be selected to remain valid. In the event of a race
              condition, one will be selected at random.
            properties:
              models:
                description: |-
                  Model defines the distinct services.
                  Model can be in 2 priority classes, Critical and Noncritical.
                  Priority class is implicitly set to Critical by specifying an Objective.
                  Otherwise the Model is considered Noncritical.
                items:
                  description: |-
                    Model defines the policies for routing the traffic of a use case, this includes performance objectives
                    and traffic splitting between different versions of the model.
                  properties:
                    name:
                      description: |-
                        The name of the model as the users set in the "model" parameter in the requests.
                        The name should be unique among the services that reference the same backend pool.
                        This is the parameter that will be used to match the request with. In the future, we may
                        allow to match on other request parameters. The other approach to support matching on
                        on other request parameters is to use a different ModelName per HTTPFilter.
                        Names can be reserved without implementing an actual model in the pool.
                        This can be done by specifying a target model and setting the weight to zero,
                        an error will be returned specifying that no valid target model is found.
                      type: string
                    objective:
                      description: |-
                        Optional
                        LLM Services with an objective have higher priority than services without.
                        IMPORTANT: By specifying an objective, this places the InferenceModel in a higher priority class than InferenceModels without a defined priority class.
                        In the face of resource-scarcity. Higher priority requests will be preserved, and lower priority class requests will be rejected.
                      properties:
                        desiredAveragePerOutputTokenLatencyAtP95OverMultipleRequests:
                          description: |-
                            The AverageLatencyPerOutputToken is calculated as the e2e request latency divided by output token
                            length. Note that this is different from what is known as TPOT (time per output token) which only
                            takes decode time into account.
                            The P95 is calculated over a fixed time window defined at the operator level.
                          format: int64
                          type: integer
                      type: object
                    targetModels:
                      description: |-
                        Optional.
                        Allow multiple versions of a model for traffic splitting.
                        If not specified, the target model name is defaulted to the modelName parameter.
                        modelName is often in reference to a LoRA adapter.
                      items:
                        description: |-
                          TargetModel represents a deployed model or a LoRA adapter. The
                          Name field is expected to match the name of the LoRA adapter
                          (or base model) as it is registered within the model server. Inference
                          Gateway assumes that the model exists on the model server and is the
                          responsibility of the user to validate a correct match. Should a model fail
                          to exist at request time, the error is processed by the Instance Gateway,
                          and then emitted on the appropriate InferenceModel object.
                        properties:
                          name:
                            description: The name of the adapter as expected by the
                              ModelServer.
                            type: string
                          weight:
                            description: |-
                              Weight is used to determine the percentage of traffic that should be
                              sent to this target model when multiple versions of the model are specified.
                            type: integer
                        type: object
                      type: array
                  type: object
                type: array
              poolRef:
                description: PoolRef are references to the backend pools that the
                  InferenceModel registers to.
                items:
                  description: ObjectReference contains enough information to let
                    you inspect or modify the referred object.
                  properties:
                    apiVersion:
                      description: API version of the referent.
                      type: string
                    fieldPath:
                      description: |-
                        If referring to a piece of an object instead of an entire object, this string
                        should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2].
                        For example, if the object reference is to a container within a pod, this would take on a value like:
                        "spec.containers{name}" (where "name" refers to the name of the container that triggered
                        the event) or if no container name is specified "spec.containers[2]" (container with
                        index 2 in this pod). This syntax is chosen only to have some well-defined way of
                        referencing a part of an object.
                      type: string
                    kind:
                      description: |-
                        Kind of the referent.
                        More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
                      type: string
                    name:
                      description: |-
                        Name of the referent.
                        More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
                      type: string
                    namespace:
                      description: |-
                        Namespace of the referent.
                        More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
                      type: string
                    resourceVersion:
                      description: |-
                        Specific resourceVersion to which this reference is made, if any.
                        More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency
                      type: string
                    uid:
                      description: |-
                        UID of the referent.
                        More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids
                      type: string
                  type: object
                  x-kubernetes-map-type: atomic
                type: array
            type: object
          status:
            description: InferenceModelStatus defines the observed state of InferenceModel
            properties:
              conditions:
                description: Conditions track the state of the InferencePool.
                items:
                  description: Condition contains details for one aspect of the current
                    state of this API Resource.
                  properties:
                    lastTransitionTime:
                      description: |-
                        lastTransitionTime is the last time the condition transitioned from one status to another.
                        This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable.
                      format: date-time
                      type: string
                    message:
                      description: |-
                        message is a human readable message indicating details about the transition.
                        This may be an empty string.
                      maxLength: 32768
                      type: string
                    observedGeneration:
                      description: |-
                        observedGeneration represents the .metadata.generation that the condition was set based upon.
                        For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date
                        with respect to the current state of the instance.
                      format: int64
                      minimum: 0
                      type: integer
                    reason:
                      description: |-
                        reason contains a programmatic identifier indicating the reason for the condition's last transition.
                        Producers of specific condition types may define expected values and meanings for this field,
                        and whether the values are considered a guaranteed API.
                        The value should be a CamelCase string.
                        This field may not be empty.
                      maxLength: 1024
                      minLength: 1
                      pattern: ^[A-Za-z]([A-Za-z0-9_,:]*[A-Za-z0-9_])?$
                      type: string
                    status:
                      description: status of the condition, one of True, False, Unknown.
                      enum:
                      - "True"
                      - "False"
                      - Unknown
                      type: string
                    type:
                      description: type of condition in CamelCase or in foo.example.com/CamelCase.
                      maxLength: 316
                      pattern: ^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$
                      type: string
                  required:
                  - lastTransitionTime
                  - message
                  - reason
                  - status
                  - type
                  type: object
                type: array
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
